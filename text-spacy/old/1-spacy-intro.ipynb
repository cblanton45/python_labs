{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : TXT-1: Exploring SpaCy\n",
    "\n",
    "## Overview\n",
    "Understand basic text processing and get to know SpaCy library\n",
    "\n",
    "## Run time \n",
    "20 min\n",
    "\n",
    "## SpaCy\n",
    "We will be using the excellent python text library called SpaCy.\n",
    "Here are some references\n",
    "* [SpaCy Home](http://www.spacy.io/)\n",
    "* [SpaCy book](https://course.spacy.io/)\n",
    "\n",
    "The following exercises will get you familiar with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SpaCy\n",
    "Let's make sure SpaCy is installed.\n",
    "You can test if SpaCy is installed by doing the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# is this working?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not you can install SpaCy as follows\n",
    "```bash\n",
    "   $   pip install spaCy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Downloading SpaCy Data\n",
    "SpaCy library comes with a set of models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the core models for English\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploring SpaCy datasets / corpus\n",
    "SpaCy ships with lots of data sets.  Here are a few\n",
    "* words\n",
    "* State of the Union\n",
    "* Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Words\n",
    "English words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000 \n",
    "\n",
    "\n",
    "words = list(nlp.vocab.strings)\n",
    "\n",
    "## TODO-2 : how many words are there?  \n",
    "## Hint : print(len(???))\n",
    "#???\n",
    "print(???(words))\n",
    "\n",
    "## TODO-2 : Print some words out\n",
    "## Hint : use w[:n] (n is a number)\n",
    "words[:???]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 State of The Union text\n",
    "Let's load some data into spaCy.  We will start with the state of the union addresses.  We will look at the 2006 State of the Union from President George W. Bush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://elephantscale-public.s3.amazonaws.com/data/text/state-of-the-unions/2006-GWBush.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## TODO-3 : Extract the speech for 2006\n",
    "gw2006 = nlp(open('2006-GWBush.txt').read())\n",
    "print (gw2006[:10])\n",
    "print(\"----\")\n",
    "\n",
    "## TODO-4 : Extract words for the same speech\n",
    "# HINT: replace ??? with gw2006\n",
    "gw2006_words = [token for token in ???]\n",
    "\n",
    "\n",
    "## TODO-5 : Print first 50 words of the speech\n",
    "#print(gw2006_words[:???])\n",
    "\n",
    "## TODO-6 : Extract sentences\n",
    "#gw2006_sents = [sentence for sentence in ???.sents]\n",
    "gw2006_sents = ???\n",
    "\n",
    "## Print first 10 sentences; HINT use [:10]\n",
    "for sentence in gw2006_sents[:???]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at hte token values for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the token values for each\n",
    "\n",
    "# Run this:\n",
    "for token in gw2006[:10]:\n",
    "    print(token.text, '\\t', token.lemma_, '\\t', token.pos_, \n",
    "          '\\t', token.tag_, '\\t', token.dep_, '\\t',\n",
    "            token.shape_, '\\t', token.is_alpha, '\\t', token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Novels!\n",
    "Let's look at some classic novels, such as [Moby Dick](https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick = nlp(open('moby-dick.txt').read())\n",
    "\n",
    "\n",
    "print (moby_dick[1:1000])\n",
    "\n",
    "## TODO-2 : Get words for Moby Dick\n",
    "#moby_dick_words = \n",
    "## TODO-3 print the number of words (Hint : len)\n",
    "## TODO-4 : print first 100 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Explore 'Movie Reviews' Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Tokenizing Text\n",
    "We are going to use spacy library to tokenize texts.  There are 3 tokenizers we are goign to test\n",
    "* nltk.tokenize.word_tokenize\n",
    "* nltk.tokenize.wordpunct_tokenize\n",
    "* nltk.tokenize.sent_tokenize\n",
    "\n",
    "Try the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"I went to Starbucks. I bought a latte for $4.50!\n",
    "Yum :-)\n",
    "\"\"\"\n",
    "\n",
    "print(\"text:\\n\", text)\n",
    "print(\"\")\n",
    "\n",
    "starbucks = nlp(text)\n",
    "starbucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Try sentence tokenization\n",
    "HINT: use `.sents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(starbucks.???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Try tokenizing on words and punctuation\n",
    "\n",
    "HINT: Just convert starbucks into a list, or do a for loop on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(???))\n",
    "print([word for word in ???])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Try getting **ONLY** words, not punctuation\n",
    "\n",
    "What we'll do is try to to get ONLY words, no punctuation.  Let's look at see how SpaCy tokenizes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the output here:\n",
    "\n",
    "for token in starbucks[:20]:\n",
    "    print(token.text, '\\t', token.lemma_, '\\t', token.pos_, \n",
    "          '\\t', token.tag_, '\\t', token.dep_, '\\t',\n",
    "            token.shape_, '\\t', token.is_alpha, '\\t', token.is_stop)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything that all the puctuation has in common? HINT: look at the `.pos_` member of the token.\n",
    "\n",
    "Can you write an expression will get all words that are not punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word for word in starbucks if word.pos_ != '???']) # TODO: Fix this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
