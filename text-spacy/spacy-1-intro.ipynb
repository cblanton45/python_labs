{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Spacy-1: Exploring SpaCy\n",
    "\n",
    "## Overview\n",
    "Understand basic text processing and get to know SpaCy library\n",
    "\n",
    "## Run time \n",
    "20 min\n",
    "\n",
    "## SpaCy\n",
    "We will be using the excellent python text library called SpaCy.\n",
    "Here are some references\n",
    "* [SpaCy Home](http://www.spacy.io/)\n",
    "* [SpaCy book](https://course.spacy.io/)\n",
    "\n",
    "The following exercises will get you familiar with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SpaCy\n",
    "Let's make sure SpaCy is installed.\n",
    "You can test if SpaCy is installed by doing the following.\n",
    "\n",
    "If not you can install SpaCy as follows\n",
    "```bash\n",
    "   $   pip install spaCy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# is this working?\n",
    "print (\"spacy version : \", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Downloading SpaCy Data\n",
    "SpaCy library comes with a set of models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the core models for English\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploring SpaCy datasets / corpus\n",
    "SpaCy ships with lots of data sets.  Here are a few\n",
    "* words\n",
    "* State of the Union\n",
    "* Novel (moby dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "print (nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Words\n",
    "English words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.max_length = 1500000 \n",
    "\n",
    "\n",
    "words = list(nlp.vocab.strings)\n",
    "\n",
    "## TODO-2 : how many words are there?  \n",
    "## Hint : print(len(???))\n",
    "print(???(words))\n",
    "\n",
    "## TODO-2 : Print some words out\n",
    "## Hint : use w[:n] (n is a number)\n",
    "print (words[:???])\n",
    "print (words[- ???:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 State of The Union text\n",
    "Let's load some data into spaCy.  We will start with the state of the union addresses.  We will look at the 2006 State of the Union from President George W. Bush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/text/state-of-the-unions/2006-GWBush.txt'\n",
    "\n",
    "## if data not found, uncomment the following\n",
    "#!wget https://elephantscale-public.s3.amazonaws.com/data/text/state-of-the-unions/2006-GWBush.txt\n",
    "#data = '2006-GWBush.txt'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## TODO-3 : Extract the speech for 2006\n",
    "gw2006 = nlp(open(data).read())\n",
    "print(\"-- gw2006 --\")\n",
    "print (gw2006[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO-4 : Extract words for the same speech\n",
    "# HINT: replace ??? with gw2006\n",
    "print (\"---- first 50 words ---\")\n",
    "gw2006_words = [token for token in ???]\n",
    "print(gw2006_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO-6 : Extract sentences\n",
    "print ('---- first 10 sentences ---')\n",
    "gw2006_sents = [sentence for sentence in gw2006.sents]\n",
    "# print (gw2006_sents[:10])\n",
    "# ## Print first 10 sentences; HINT use [:10]\n",
    "for sentence in gw2006_sents[:???]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the token values for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the token values for each\n",
    "\n",
    "# Run this:\n",
    "for token in gw2006[:10]:\n",
    "    print (\"token.text : \", token.text)\n",
    "    print (\"    token.lemma_ : \", token.lemma_)\n",
    "    print (\"    token.pos_ : \", token.pos_)\n",
    "    print (\"    token.tag_ : \", token.tag_)\n",
    "    print (\"    token.dep_ : \", token.dep_)\n",
    "    print (\"    token.shape_ : \", token.shape_)\n",
    "    print (\"    token.is_alpha : \", token.is_alpha)\n",
    "    print (\"    token.is_stop : \", token.is_stop)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Novels!\n",
    "Let's look at some classic novels, such as [Moby Dick](https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt)\n",
    "\n",
    "We need more memory for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/text/books/moby-dick.txt'\n",
    "\n",
    "## if data not found, uncomment the following\n",
    "# !wget https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt\n",
    "#data = 'moby-dick.txt'   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "moby_dick = nlp(open(data).read())\n",
    "\n",
    "# print(moby_dick)\n",
    "# print (moby_dick[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-2 : Get words for Moby Dick\n",
    "# moby_dick_words = \n",
    "# TODO-3 print the number of words (Hint : len)\n",
    "# TODO-4 : print first 100 words\n",
    "\n",
    "moby_dick_words = [token for token in ???]\n",
    "\n",
    "print(\"len(words) : \", len(moby_dick_words))\n",
    "print (\"---- first 100 words ---\")\n",
    "print(moby_dick_words[:???])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Tokenizing Your Own Text\n",
    "We are going to use spacy library to tokenize texts.  There are 3 tokenizers we are goign to test\n",
    "\n",
    "Try the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : feel free to change this to your own text\n",
    "text=\"\"\"I went to Starbucks. I bought a latte for $4.50!\n",
    "Yum :-)\n",
    "\"\"\"\n",
    "\n",
    "print(\"text:\\n\", text)\n",
    "print(\"\")\n",
    "\n",
    "starbucks = nlp(text)\n",
    "starbucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Try sentence tokenization\n",
    "HINT: use `.sents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(starbucks.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Try tokenizing on words and punctuation\n",
    "\n",
    "HINT: Just convert starbucks into a list, or do a for loop on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(starbucks))\n",
    "print([word for word in starbucks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Try getting **ONLY** words, not punctuation\n",
    "\n",
    "What we'll do is try to to get ONLY words, no punctuation.  Let's look at see how SpaCy tokenizes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the output here:\n",
    "\n",
    "for token in starbucks[:20]:\n",
    "    print (\"token.text : \", token.text)\n",
    "    print (\"    token.lemma_ : \", token.lemma_)\n",
    "    print (\"    token.pos_ : \", token.pos_)\n",
    "    print (\"    token.tag_ : \", token.tag_)\n",
    "    print (\"    token.dep_ : \", token.dep_)\n",
    "    print (\"    token.shape_ : \", token.shape_)\n",
    "    print (\"    token.is_alpha : \", token.is_alpha)\n",
    "    print (\"    token.is_stop : \", token.is_stop)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything that all the puctuation has in common? HINT: look at the `.pos_` member of the token.\n",
    "\n",
    "Can you write an expression will get all words that are not punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word for word in starbucks if word.pos_ != '???']) # TODO: Fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize The Sentence\n",
    "We will use `displacy` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(starbucks, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display sentences individually\n",
    "\n",
    "displacy.render(list(starbucks.sents), style=\"dep\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
